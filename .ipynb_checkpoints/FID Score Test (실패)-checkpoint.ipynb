{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a259a7e",
   "metadata": {},
   "source": [
    "# 첫번째 시도 - 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c619e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import inception_v3\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "\n",
    "# 평균과 공분산 계산 함수\n",
    "def calculate_activation_statistics(x, model, device):\n",
    "    # Inception 모델의 중간층(layer)까지 모델 정의\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    upsample = torch.nn.Upsample(size=(299, 299), mode='bilinear').to(device)\n",
    "    block_idx = inception_v3.BLOCK_INDEX_BY_DIM[2048]\n",
    "    model = torch.nn.Sequential(\n",
    "        *list(model.children())[:block_idx+1]\n",
    "    )\n",
    "\n",
    "    # 데이터셋의 데이터를 Inception 모델에 입력하여 중간층의 출력값을 추출\n",
    "    n = x.shape[0]\n",
    "    x = upsample(x)\n",
    "    with torch.no_grad():\n",
    "        x = model(x.to(device))\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "    # 평균과 공분산 계산\n",
    "    mean = torch.mean(x, dim=0)\n",
    "    cov = torch.matmul(x.T, x) / n - torch.matmul(mean.T, mean)\n",
    "    return mean.cpu().numpy(), cov.cpu().numpy()\n",
    "\n",
    "# FID 계산 함수\n",
    "def calculate_fid(real_images, fake_images, model, device):\n",
    "    # 실제 데이터셋에 대한 평균과 공분산 계산\n",
    "    m1, s1 = calculate_activation_statistics(real_images, model, device)\n",
    "\n",
    "    # 생성된 데이터셋에 대한 평균과 공분산 계산\n",
    "    m2, s2 = calculate_activation_statistics(fake_images, model, device)\n",
    "\n",
    "    # 평균 제곱근 차이와 공분산 합계의 제곱근 계산\n",
    "    diff = m1 - m2\n",
    "    covmean, _ = linalg.sqrtm(s1.dot(s2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        offset = np.eye(s1.shape[0]) * 1e-6\n",
    "        covmean = linalg.sqrtm((s1 + offset).dot(s2 + offset))\n",
    "    fid = np.dot(diff, diff) + np.trace(s1 + s2 - 2 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ebde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Inception V3 모델 불러오기\n",
    "model = inception_v3(pretrained=True, transform_input=False, aux_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e772b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_images와 fake_images를 텐서 형태로 준비\n",
    "# 이 텐서들은 적절한 전처리를 거친 후에 생성되어야 함\n",
    "real_images = torch.randn(32, 3, 299, 299)  # 예시를 위해 무작위 텐서 사용\n",
    "fake_images = torch.randn(32, 3, 299, 299)  # 예시를 위해 무작위 텐서 사용\n",
    "\n",
    "# FID 값을 계산\n",
    "fid = calculate_fid(real_images, fake_images, model, device)\n",
    "print(\"FID score:\", fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf0a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05c637f4",
   "metadata": {},
   "source": [
    "# 두번째 시도 - 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc90ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import inception_v3\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "\n",
    "# 평균과 공분산 계산 함수\n",
    "def calculate_activation_statistics(x, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    upsample = torch.nn.Upsample(size=(299, 299), mode='bilinear').to(device)\n",
    "\n",
    "    # 데이터셋의 데이터를 Inception 모델에 입력하여 중간층의 출력값을 추출\n",
    "    n = x.shape[0]\n",
    "    x = upsample(x)\n",
    "    with torch.no_grad():\n",
    "        x = model(x.to(device))  # 주목: 중간층의 출력값만 사용\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze(-1).squeeze(-1)\n",
    "\n",
    "    # 평균과 공분산 계산\n",
    "    mean = torch.mean(x, dim=0)\n",
    "    cov = torch.matmul(x.T, x) / n - torch.matmul(mean.T, mean)\n",
    "    return mean.cpu().numpy(), cov.cpu().numpy()\n",
    "\n",
    "\n",
    "# FID 계산 함수\n",
    "def calculate_fid(real_images, fake_images, model, device):\n",
    "    # 실제 데이터셋에 대한 평균과 공분산 계산\n",
    "    m1, s1 = calculate_activation_statistics(real_images, model, device)\n",
    "\n",
    "    # 생성된 데이터셋에 대한 평균과 공분산 계산\n",
    "    m2, s2 = calculate_activation_statistics(fake_images, model, device)\n",
    "\n",
    "    # 평균 제곱근 차이와 공분산 합계의 제곱근 계산\n",
    "    diff = m1 - m2\n",
    "    covmean, _ = linalg.sqrtm(s1.dot(s2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        offset = np.eye(s1.shape[0]) * 1e-6\n",
    "        covmean = linalg.sqrtm((s1 + offset).dot(s2 + offset))\n",
    "    fid = np.dot(diff, diff) + np.trace(s1 + s2 - 2 * covmean)\n",
    "    return fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88428aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Inception V3 모델 불러오기\n",
    "model = inception_v3(pretrained=True, transform_input=False, aux_logits=True)\n",
    "\n",
    "# Inception 모델의 중간층(layer)까지 모델 정의\n",
    "# 여기에서, inception_v3에서 미리 정의된 중간층 인덱스를 사용할 수 없으므로,\n",
    "# 미리 정의된 인덱스 값인 9를 직접 사용합니다.\n",
    "model = torch.nn.Sequential(\n",
    "    *list(model.children())[:9+1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_images와 fake_images를 텐서 형태로 준비\n",
    "# 이 텐서들은 적절한 전처리를 거친 후에 생성되어야 함\n",
    "real_images = torch.randn(32, 3, 299, 299)  # 예시를 위해 무작위 텐서 사용\n",
    "fake_images = torch.randn(32, 3, 299, 299)  # 예시를 위해 무작위 텐서 사용\n",
    "\n",
    "# FID 값을 계산\n",
    "fid = calculate_fid(real_images, fake_images, model, device)\n",
    "print(\"FID score:\", fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c87897",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e213d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# 입력 이미지의 크기가 256x256인 경우\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "path = \"/SSD3_8TB/Daniel/06_pGAN/pGAN-cGAN/results/1_pGAN_run_align/test_latest/images/IXI0_fake_B.png\"\n",
    "# 이미지 불러오기\n",
    "image = Image.open(path).convert('RGB')\n",
    "# 이미지를 tensor로 변환하기\n",
    "fake_images = torch.tensor(np.array(image)).permute(2, 0, 1).unsqueeze(0).float()\n",
    "# 출력\n",
    "print(fake_images.shape)\n",
    "\n",
    "path = \"/SSD3_8TB/Daniel/06_pGAN/pGAN-cGAN/results/1_pGAN_run_align/test_latest/images/IXI0_real_B.png\"\n",
    "# 이미지 불러오기\n",
    "image = Image.open(path).convert('RGB')\n",
    "# 이미지를 tensor로 변환하기\n",
    "real_images = torch.tensor(np.array(image)).permute(2, 0, 1).unsqueeze(0).float()\n",
    "# 출력\n",
    "print(real_images.shape)\n",
    "\n",
    "# FID 값을 계산\n",
    "fid = calculate_fid(real_images, fake_images, model, device)\n",
    "print(\"FID score:\", fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# 입력 이미지의 크기가 256x256인 경우\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "image = Image.open(path).convert('RGB')\n",
    "fake_images = transform(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c61b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f77eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# 입력 이미지의 크기가 256x256인 경우\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "path = \"/SSD3_8TB/Daniel/06_pGAN/pGAN-cGAN/results/1_pGAN_run_align/test_latest/images/IXI0_real_B.png\"\n",
    "\n",
    "image = Image.open(path).convert('RGB')\n",
    "fake_images = transform(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e873d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fake_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0780f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62159757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe7f53f9",
   "metadata": {},
   "source": [
    "# 세번째 시도 - 실패\n",
    "출처: https://github.com/mseitzer/pytorch-fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae71e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-fid\n",
      "  Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: scipy in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from pytorch-fid) (1.9.1)\n",
      "Requirement already satisfied: torchvision>=0.2.2 in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from pytorch-fid) (0.14.1)\n",
      "Requirement already satisfied: numpy in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from pytorch-fid) (1.23.5)\n",
      "Requirement already satisfied: torch>=1.0.1 in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from pytorch-fid) (1.13.1)\n",
      "Requirement already satisfied: pillow in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from pytorch-fid) (9.4.0)\n",
      "Requirement already satisfied: typing_extensions in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from torch>=1.0.1->pytorch-fid) (4.5.0)\n",
      "Requirement already satisfied: requests in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from torchvision>=0.2.2->pytorch-fid) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from requests->torchvision>=0.2.2->pytorch-fid) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from requests->torchvision>=0.2.2->pytorch-fid) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from requests->torchvision>=0.2.2->pytorch-fid) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/milab/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages (from requests->torchvision>=0.2.2->pytorch-fid) (1.26.14)\n",
      "Installing collected packages: pytorch-fid\n",
      "Successfully installed pytorch-fid-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "829ea488",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in /SSD3_8TB/Daniel/06_pGAN/pGAN-cGAN/results/1_pGAN_run_align/test_latest/images/fake.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m fake_images_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/SSD3_8TB/Daniel/06_pGAN/pGAN-cGAN/results/1_pGAN_run_align/test_latest/images/real\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 이미지 데이터셋 생성\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m real_images_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreal_images_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCenterCrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m fake_images_dataset \u001b[38;5;241m=\u001b[39m dset\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39mfake_images_path,\n\u001b[1;32m     23\u001b[0m                                        transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     24\u001b[0m                                            transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m                                                                 std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m])\n\u001b[1;32m     29\u001b[0m                                        ]))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 데이터로더 생성\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/danny_py3.8.8/lib/python3.8/site-packages/torchvision/datasets/folder.py:42\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /SSD3_8TB/Daniel/06_pGAN/pGAN-cGAN/results/1_pGAN_run_align/test_latest/images/fake."
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from pytorch_fid import fid_score\n",
    "\n",
    "# 실제 이미지 폴더 경로\n",
    "real_images_path = \"/SSD3_8TB/Daniel/06_pGAN/pGAN-cGAN/results/1_pGAN_run_align/test_latest/images/real\"\n",
    "\n",
    "# 생성된 이미지 폴더 경로\n",
    "fake_images_path = \"/SSD3_8TB/Daniel/06_pGAN/pGAN-cGAN/results/1_pGAN_run_align/test_latest/images/fake\"\n",
    "\n",
    "# 이미지 데이터셋 생성\n",
    "real_images_dataset = dset.ImageFolder(root=real_images_path,\n",
    "                                       transform=transforms.Compose([\n",
    "                                           transforms.Resize(256),\n",
    "                                           transforms.CenterCrop(256),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                                std=[0.5, 0.5, 0.5])\n",
    "                                       ]))\n",
    "fake_images_dataset = dset.ImageFolder(root=fake_images_path,\n",
    "                                       transform=transforms.Compose([\n",
    "                                           transforms.Resize(256),\n",
    "                                           transforms.CenterCrop(256),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                                std=[0.5, 0.5, 0.5])\n",
    "                                       ]))\n",
    "\n",
    "# 데이터로더 생성\n",
    "real_images_dataloader = torch.utils.data.DataLoader(real_images_dataset,\n",
    "                                                      batch_size=32,\n",
    "                                                      shuffle=True,\n",
    "                                                      num_workers=8)\n",
    "fake_images_dataloader = torch.utils.data.DataLoader(fake_images_dataset,\n",
    "                                                      batch_size=32,\n",
    "                                                      shuffle=True,\n",
    "                                                      num_workers=8)\n",
    "\n",
    "# FID 계산\n",
    "fid_value = fid_score.calculate_fid_given_paths([real_images_dataloader, fake_images_dataloader], batch_size=32, device='cuda')\n",
    "print('FID score:', fid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fc9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danny_py3.8.8",
   "language": "python",
   "name": "danny_py3.8.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
